# /docker-compose.yml (Definitive, Resilient Version)
services:
  nginx:
    image: nginx:latest
    container_name: corpus_nginx
    ports: ["80:80"]
    volumes: ["./nginx.conf:/etc/nginx/conf.d/default.conf:ro"]
    depends_on: [web]

  web:
    build: ./server
    container_name: corpus_web_app
    env_file: .env
    volumes: ["./server:/app", "corpus_uploads:/app/uploads"]
    depends_on:
      db: { condition: service_healthy }
      redis: { condition: service_healthy }
    command: >
      sh -c "flask init-db && 
             gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 'app:create_app()'"

  worker:
    build: ./server
    container_name: corpus_worker
    env_file: .env
    volumes: ["./server:/app", "corpus_uploads:/app/uploads"]
    depends_on:
      redis: { condition: service_healthy }
      db: { condition: service_healthy }
      tika: { condition: service_healthy }
      chroma: { condition: service_healthy }
      ollama: { condition: service_healthy }
    command: celery -A app.celery worker --loglevel=info # <<< FIX: Points to the celery instance in app.py

  redis:
    image: redis:7-alpine
    container_name: corpus_redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  tika:
    build: 
      context: .
      dockerfile: tika.Dockerfile # <<< OPTIMIZATION: Use custom, smaller Tika image
    container_name: corpus_tika
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9998/tika"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s

  db:
    image: postgres:15
    container_name: corpus_db
    env_file: .env
    volumes: ["corpus_data:/var/lib/postgresql/data"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5

  chroma:
    image: chromadb/chroma:0.4.24
    container_name: corpus_chroma
    # FIX 1: The volume now points to the parent directory for simplicity.
    volumes: ["chroma_data:/chroma"]
    # FIX 2: Replaced IS_PERSISTENT with an explicit command to run the server.
    # This is the modern, reliable way to ensure persistence.
    command: ["--host", "0.0.0.0", "--port", "8000", "--path", "/chroma"]
    environment:
      - ANONYMIZED_TELEMETRY=False 
      # IS_PERSISTENT is no longer needed
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 10s
      timeout: 5s
      retries: 5

  ollama:
    # FIX 1: Revert to the standard, official image. No 'build' section needed.
    image: ollama/ollama
    container_name: corpus_ollama
    volumes: ["ollama_data:/root/.ollama"]
    tty: true
    # FIX 2: Use a robust startup command.
    # This command runs inside the container on startup. It checks if 'tinyllama'
    # is already downloaded. If not, it pulls it. Then, it starts the server.
    # This is far more reliable than building the model into the image.
    command: >
      sh -c "
      (ollama list | grep -q tinyllama || ollama pull tinyllama) &&
      ollama serve
      "
    environment:
      # This is still useful for CPU compatibility.
      - OLLAMA_LLM_LIBRARY=/usr/local/lib/ollama/llm/cpu_avx_off
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 5
      # Give the container more time to start, as it might be downloading the model.
      start_period: 300s

volumes:
  corpus_data: {}
  chroma_data: {}
  ollama_data: {}
  corpus_uploads: {}