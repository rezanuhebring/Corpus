# /docker-compose.yml (Definitive, Resilient Version)
services:
  nginx:
    image: nginx:latest
    container_name: corpus_nginx
    ports: ["80:80"]
    volumes: ["./nginx.conf:/etc/nginx/conf.d/default.conf:ro"]
    depends_on: [web]

  web:
    build: ./server
    container_name: corpus_web_app
    env_file: .env
    volumes: ["./server:/app", "corpus_uploads:/app/uploads"]
    depends_on:
      db: { condition: service_healthy }
      redis: { condition: service_healthy }
    command: >
      sh -c "flask init-db && 
             gunicorn --bind 0.0.0.0:5000 --workers 2 --timeout 120 'app:create_app()'"

  worker:
    build: ./server
    container_name: corpus_worker
    env_file: .env
    volumes: ["./server:/app", "corpus_uploads:/app/uploads"]
    depends_on:
      redis: { condition: service_healthy }
      db: { condition: service_healthy }
      tika: { condition: service_healthy }
      chroma: { condition: service_healthy }
      ollama: { condition: service_healthy }
    command: celery -A app.celery worker --loglevel=info # <<< FIX: Points to the celery instance in app.py

  redis:
    image: redis:7-alpine
    container_name: corpus_redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  tika:
    build: 
      context: .
      dockerfile: tika.Dockerfile # <<< OPTIMIZATION: Use custom, smaller Tika image
    container_name: corpus_tika
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9998/tika"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s

  db:
    image: postgres:15
    container_name: corpus_db
    env_file: .env
    volumes: ["corpus_data:/var/lib/postgresql/data"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5

  # In your docker-compose.yml file:

  chroma:
    image: chromadb/chroma:0.4.24
    container_name: corpus_chroma
    # The volume mount is correct, we are mounting to /chroma
    volumes: ["chroma_data:/chroma"]
    # THIS IS THE DEFINITIVE FIX:
    # We override the command to first fix permissions, then run the server.
    command: >
      sh -c "
      chown -R chroma:chroma /chroma &&
      chroma run --host 0.0.0.0 --port 8000 --path /chroma
      "
    # No environment variables like IS_PERSISTENT are needed. The command handles everything.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 10s
      timeout: 5s
      retries: 5
      # Give it a generous start period to allow for the chown and server init.
      start_period: 30s

  ollama:
    # FIX: The container's only job is to run the server. No complex command.
    image: ollama/ollama
    container_name: corpus_ollama
    volumes: ["ollama_data:/root/.ollama"]
    tty: true
    environment:
      # This is still useful for CPU compatibility.
      - OLLAMA_LLM_LIBRARY=/usr/local/lib/ollama/llm/cpu_avx_off
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # FIX: Re-introducing the model puller. This is the robust, decoupled way.
  ollama-model-puller:
    image: curlimages/curl:latest
    container_name: corpus_model_puller
    command: >
      /bin/sh -c "
      echo 'Waiting for Ollama service to be ready...';
      until curl -s -f http://ollama:11434/ > /dev/null; do
        echo -n '.' && sleep 2;
      done;
      echo '\nOllama is ready. Pulling model \"tinyllama\"...';
      curl -v http://ollama:11434/api/pull -d '{ \"name\": \"tinyllama\" }';
      echo 'Model pull command sent. This container will now exit.';
      "
    # This just needs the ollama service to have started, not be healthy.
    depends_on:
      - ollama

volumes:
  corpus_data: {}
  chroma_data: {}
  ollama_data: {}
  corpus_uploads: {}